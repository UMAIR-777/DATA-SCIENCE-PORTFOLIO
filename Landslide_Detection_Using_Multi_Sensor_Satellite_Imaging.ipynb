{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UMAIR-777/DATA-SCIENCE-PORTFOLIO/blob/main/Landslide_Detection_Using_Multi_Sensor_Satellite_Imaging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Landslides are a natural phenomenon with devastating consequences, frequent in many parts of the world. Thousands of small and medium-sized ground movements follow earthquakes or heavy rainfalls. Landslides have become more damaging in recent years due to climate change, population growth, and unplanned urbanization in unstable mountain areas. Early landslide detection is critical for quick response and management of the consequences. Accurate detection provides information on the landslide's exact location and extent, which is necessary for landslide susceptibility modelling and risk assessment."
      ],
      "metadata": {
        "id": "CbKo0mqWnFbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "Wh8Jlv_ZZmYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mport pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "QDIO82PnZmbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "metadata": {
        "id": "ADrvrSvKb9Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the dataset\n",
        "path_single = r\"data/img/image_2000.h5\"\n",
        "path_single_mask = r'data/mask/mask_2000.h5'"
      ],
      "metadata": {
        "id": "VkAc0O3eZme2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(r'/content/gdrive/MyDrive/landslide4Sense')"
      ],
      "metadata": {
        "id": "oFVak0mZZmg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the dataset¶"
      ],
      "metadata": {
        "id": "AXfzypapcvKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the dataset\n",
        "path_single = r\"data/img/image_2000.h5\"\n",
        "path_single_mask = r'data/mask/mask_2000.h5'"
      ],
      "metadata": {
        "id": "CjoCRwtbZmmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_data = np.zeros((1, 128,128, 3))\n",
        "with h5py.File(path_single) as hdf:\n",
        "    ls = list(hdf.keys())\n",
        "    print(\"ls\", ls)\n",
        "    data = np.array(hdf.get('img'))\n",
        "    print(\"input data shape:\", data.shape)\n",
        "    plt.imshow(data[:, :, 3:0:-1])\n",
        "\n",
        "    data_red = data[:, :, 3]\n",
        "    data_green = data[:, :, 2]\n",
        "    data_blue = data[:, :, 1]\n",
        "    data_nir = data[:, :, 7]\n",
        "    data_rgb = data[:, :, 3:0:-1]\n",
        "    data_ndvi = np.divide(data_nir - data_red,np.add(data_nir, data_red))\n",
        "    f_data[0, :, :, 0] =data_ndvi\n",
        "    f_data[0, :, :, 1] = data[:, :, 12]\n",
        "    f_data[0, :, :, 2] = data[:, :, 13]\n",
        "\n",
        "    print(\"data ndvi shape \", data_ndvi.shape, \"f_data shape: \", f_data.shape)\n",
        "    plt.imshow(data_ndvi)"
      ],
      "metadata": {
        "id": "S0cOD5tQZmo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File(path_single_mask) as hdf:\n",
        "    ls = list(hdf.keys())\n",
        "    print(\"ls\", ls)\n",
        "    data = np.array(hdf.get('mask'))\n",
        "    print(\"input data shape:\", data.shape)\n",
        "    plt.imshow(data)"
      ],
      "metadata": {
        "id": "m3O9Ka2UZmyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using all dataset"
      ],
      "metadata": {
        "id": "aGaMm9l6dOrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_single = r\"data/img/image_10.h5\"\n",
        "path_single_mask = r'data/mask/mask_1.h5'\n",
        "TRAIN_PATH = r\"data/img/*.h5\"\n",
        "TRAIN_MASK = r'data/mask/*.h5'\n",
        "\n",
        "TRAIN_XX = np.zeros((3799, 128, 128, 6))\n",
        "TRAIN_YY = np.zeros((3799, 128, 128, 1))\n",
        "all_train = sorted(glob.glob(TRAIN_PATH))\n",
        "all_mask = sorted(glob.glob(TRAIN_MASK))"
      ],
      "metadata": {
        "id": "xINCBFKDZm0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train with RGB, NDVI, DEM, and Slope"
      ],
      "metadata": {
        "id": "tKL7xiJzdW3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#testing for google colab GPU\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "3Nj55cPIdo0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w4z9hozSeaHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (img, mask) in enumerate(zip(all_train, all_mask)):\n",
        "    print(i, img, mask)\n",
        "    with h5py.File(img) as hdf:\n",
        "        ls = list(hdf.keys())\n",
        "        data = np.array(hdf.get('img'))\n",
        "\n",
        "        # assign 0 for the nan value\n",
        "        data[np.isnan(data)] = 0.000001\n",
        "\n",
        "        # to normalize the data\n",
        "        mid_rgb = data[:, :, 1:4].max() / 2.0\n",
        "        mid_slope = data[:, :, 12].max() / 2.0\n",
        "        mid_elevation = data[:, :, 13].max() / 2.0\n",
        "\n",
        "        # ndvi calculation\n",
        "        data_red = data[:, :, 3]\n",
        "        data_nir = data[:, :, 7]\n",
        "        data_ndvi = np.divide(data_nir - data_red,np.add(data_nir, data_red))\n",
        "\n",
        "        # final array\n",
        "        TRAIN_XX[i, :, :, 0] = 1 - data[:, :, 3] / mid_rgb  #RED\n",
        "        TRAIN_XX[i, :, :, 1] = 1 - data[:, :, 2] / mid_rgb #GREEN\n",
        "        TRAIN_XX[i, :, :, 2] = 1 - data[:, :, 1] / mid_rgb #BLUE\n",
        "        TRAIN_XX[i, :, :, 3] = data_ndvi #NDVI\n",
        "        TRAIN_XX[i, :, :, 4] = 1 - data[:, :, 12] / mid_slope #SLOPE\n",
        "        TRAIN_XX[i, :, :, 5] = 1 - data[:, :, 13] / mid_elevation #ELEVATION\n",
        "\n",
        "\n",
        "    with h5py.File(mask) as hdf:\n",
        "        ls = list(hdf.keys())\n",
        "        data=np.array(hdf.get('mask'))\n",
        "        TRAIN_YY[i, :, :, 0] = data\n"
      ],
      "metadata": {
        "id": "iZU5vzjWemOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing min, max values in train data%"
      ],
      "metadata": {
        "id": "gsrDcgOheuh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " TRAIN_XX_n = TRAIN_XX / TRAIN_XX.max()\n",
        "TRAIN_XX[np.isnan(TRAIN_XX)] = 0.000001\n",
        "print(TRAIN_XX.min(), TRAIN_XX.max(), TRAIN_YY.min(), TRAIN_YY.max())\n"
      ],
      "metadata": {
        "id": "t5VIKCOOevSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "  y_true = tf.cast(y_true, tf.float32)\n",
        "  y_pred = tf.math.sigmoid(y_pred)\n",
        "  numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
        "  denominator = tf.reduce_sum(y_true + y_pred)\n",
        "\n",
        "  return 1 - numerator / denominator"
      ],
      "metadata": {
        "id": "9YQrpeFyfPBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lr7Tg0P9few1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img=234\n",
        "fig,(ax1,ax2, ax3, ax4, ax5)= plt.subplots(1,5,figsize=(15,10))\n",
        "\n",
        "\n",
        "ax1.set_title(\"RGB image\")\n",
        "ax2.set_title(\"NDVI\")\n",
        "ax3.set_title(\"Slope\")\n",
        "ax4.set_title(\"Elevation\")\n",
        "ax5.set_title(\"Mask\")\n",
        "ax1.imshow(TRAIN_XX[img, :, :, 0:3])\n",
        "ax2.imshow(TRAIN_XX[img, :, :, 3])\n",
        "ax3.imshow(TRAIN_XX[img, :, :, 4])\n",
        "ax4.imshow(TRAIN_XX[img, :, :, 5])\n",
        "ax5.imshow(TRAIN_YY[img, :, :, 0])"
      ],
      "metadata": {
        "id": "55OaAl20fjMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "G97-C0BwgJU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img=234\n",
        "fig,(ax1,ax2, ax3, ax4, ax5)= plt.subplots(1,5,figsize=(15,10))\n",
        "\n",
        "\n",
        "ax1.set_title(\"RGB image\")\n",
        "ax2.set_title(\"NDVI\")\n",
        "ax3.set_title(\"Slope\")\n",
        "ax4.set_title(\"Elevation\")\n",
        "ax5.set_title(\"Mask\")\n",
        "ax1.imshow(TRAIN_XX[img, :, :, 0:3])\n",
        "ax2.imshow(TRAIN_XX[img, :, :, 3])\n",
        "ax3.imshow(TRAIN_XX[img, :, :, 4])\n",
        "ax4.imshow(TRAIN_XX[img, :, :, 5])\n",
        "ax5.imshow(TRAIN_YY[img, :, :, 0])"
      ],
      "metadata": {
        "id": "jsFjD5DiggTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(TRAIN_XX, TRAIN_YY, test_size=0.2, shuffle= True)"
      ],
      "metadata": {
        "id": "nvc8upQjg21X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img=1545\n",
        "fig,(ax1,ax2, ax3, ax4, ax5)= plt.subplots(1,5,figsize=(15,10))\n",
        "\n",
        "ax1.set_title(\"RGB image\")\n",
        "ax2.set_title(\"NDVI\")\n",
        "ax3.set_title(\"Slope\")\n",
        "ax4.set_title(\"Elevation\")\n",
        "ax5.set_title(\"Mask\")\n",
        "ax1.imshow(x_train[img, :, :, 0:3])\n",
        "ax2.imshow(x_train[img, :, :, 3])\n",
        "ax3.imshow(x_train[img, :, :, 4])\n",
        "ax4.imshow(x_train[img, :, :, 5])\n",
        "ax5.imshow(y_train[img, :, :, 0])"
      ],
      "metadata": {
        "id": "4SQqB1UihRJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape, y_train.shape\n",
        "# to release some memory, delete the unnecessary variable\n",
        "del TRAIN_XX\n",
        "del TRAIN_YY\n",
        "del all_train\n",
        "del all_mask\n"
      ],
      "metadata": {
        "id": "Pj9MqhVDhocn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNET MODEL"
      ],
      "metadata": {
        "id": "YQzdrsaHh6L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import recall_m, precision_m, f1_m\n",
        "def unet_model(IMG_WIDTH, IMG_HIGHT, IMG_CHANNELS):\n",
        "    inputs = tf.keras.layers.Input((IMG_WIDTH, IMG_HIGHT, IMG_CHANNELS))\n",
        "\n",
        "    # Converted inputs to floating\n",
        "    #s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "\n",
        "\n",
        "    #Contraction path\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n",
        "    c1 = tf.keras.layers.Dropout(0.1)(c1)\n",
        "    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = tf.keras.layers.Dropout(0.1)(c2)\n",
        "    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "    c3 = tf.keras.layers.Dropout(0.2)(c3)\n",
        "    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "    c4 = tf.keras.layers.Dropout(0.2)(c4)\n",
        "    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "    c5 = tf.keras.layers.Dropout(0.3)(c5)\n",
        "    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "    #Expansive path\n",
        "    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "    c6 = tf.keras.layers.Dropout(0.2)(c6)\n",
        "    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = tf.keras.layers.Dropout(0.2)(c7)\n",
        "    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "    c8 = tf.keras.layers.Dropout(0.1)(c8)\n",
        "    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
        "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "    c9 = tf.keras.layers.Dropout(0.1)(c9)\n",
        "    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Qal4ai7Wh887"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = unet_model(128, 128, 6)\n",
        "# model.summary()\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(\"best_model.h5\", monitor=\"val_f1_m\", verbose=1, save_best_only=True, mode=\"max\")\n",
        "# earlyStopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_m', patience=10, verbose=1, mode='max')\n",
        "\n",
        "callbacks = [\n",
        "    # earlyStopping,\n",
        "    checkpointer\n",
        "    ]\n",
        "history = model.fit(x_train, y_train, batch_size=16,\n",
        "          epochs=100,\n",
        "          verbose = 2,\n",
        "          validation_data=(x_valid, y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model.save(\"model_save.h5\")\n",
        "loss, accuracy, f1_score, precision, recall = model.evaluate(x_valid, y_valid, verbose=0)\n",
        "print(loss, accuracy, f1_score, precision, recall)"
      ],
      "metadata": {
        "id": "ghsmS8OHiK0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,((ax11,ax12),(ax13,ax14)) = plt.subplots(2,2,figsize=(20,15))\n",
        "ax11.plot(history.history['loss'])\n",
        "ax11.plot(history.history['val_loss'])\n",
        "ax11.title.set_text('Unet model loss')\n",
        "ax11.set_ylabel('loss')\n",
        "ax11.set_xlabel('epoch')\n",
        "ax11.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "ax12.plot(history.history['precision_m'])\n",
        "ax12.plot(history.history['val_precision_m'])\n",
        "ax12.set_title('Unet model precision')\n",
        "ax12.set_ylabel('precision')\n",
        "ax12.set_xlabel('epoch')\n",
        "ax12.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "ax13.plot(history.history['recall_m'])\n",
        "ax13.plot(history.history['val_recall_m'])\n",
        "ax13.set_title('Unet model recall')\n",
        "ax13.set_ylabel('recall')\n",
        "ax13.set_xlabel('epoch')\n",
        "ax13.legend(['train', 'validation'], loc='upper left')\n",
        "\n",
        "ax14.plot(history.history['f1_m'])\n",
        "ax14.plot(history.history['val_f1_m'])\n",
        "ax14.set_title('Unet model f1')\n",
        "ax14.set_ylabel('f1')\n",
        "ax14.set_xlabel('epoch')\n",
        "ax14.legend(['train', 'validation'], loc='upper left')\n",
        "threshold = 0.5\n",
        "pred_img = mod"
      ],
      "metadata": {
        "id": "KBV5bd7TiYza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5\n",
        "pred_img = model.predict(x_valid)\n",
        "pred_img = (pred_img > threshold).astype(np.uint8)"
      ],
      "metadata": {
        "id": "UgONCNzGikFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = 155\n",
        "fig,(ax1,ax2,ax3)= plt.subplots(1,3,figsize=(15,10))\n",
        "ax1.imshow(pred_img[img, :, :, 0])\n",
        "ax1.set_title(\"Predictions\")\n",
        "ax2.imshow(y_valid[img, :, :, 0])\n",
        "ax2.set_title(\"Label\")\n",
        "ax3.imshow(x_valid[img, :, :, 0:3])\n",
        "ax3.set_title('Training Image')"
      ],
      "metadata": {
        "id": "98hq0LGZiqoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validation data¶**"
      ],
      "metadata": {
        "id": "wuFhIaHXis4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_url = r'/content/gdrive/MyDrive/DL/landslide4Sense/data/validation/img/*.h5'\n",
        "img_val = sorted(glob.glob(validation_url))\n",
        "\n",
        "VAL_XX = np.zeros((245, 128, 128, 6))\n",
        "mask_name = []\n",
        "for i, img in enumerate(img_val):\n",
        "    print(i, img)\n",
        "    mask_name.append(img.split('/')[-1].replace('image', 'mask'))\n",
        "    with h5py.File(img) as hdf:\n",
        "        ls = list(hdf.keys())\n",
        "        data = np.array(hdf.get('img'))\n",
        "\n",
        "        # assign 0 for the nan value\n",
        "        data[np.isnan(data)] = 0.000001\n",
        "\n",
        "        # to normalize the data\n",
        "        mid_rgb = data[:, :, 1:4].max() / 2.0\n",
        "        mid_slope = data[:, :, 12].max() / 2.0\n",
        "        mid_elevation = data[:, :, 13].max() / 2.0\n",
        "\n",
        "        # ndvi calculation\n",
        "        data_red = data[:, :, 3]\n",
        "        data_nir = data[:, :, 7]\n",
        "        data_ndvi = np.divide(data_nir - data_red,np.add(data_nir, data_red))\n",
        "\n",
        "        # final array\n",
        "        VAL_XX[i, :, :, 0] = 1 - data[:, :, 3] / mid_rgb #RED\n",
        "        VAL_XX[i, :, :, 1] = 1 - data[:, :, 2] / mid_rgb #GREEN\n",
        "        VAL_XX[i, :, :, 2] = 1 - data[:, :, 1] / mid_rgb #BLUE\n",
        "        VAL_XX[i, :, :, 3] = data_ndvi #NDVI\n",
        "        VAL_XX[i, :, :, 4] = 1- data[:, :, 13] / mid_slope #SLOPE\n",
        "        VAL_XX[i, :, :, 5] = 1 - data[:, :, 13] / mid_elevation #ELEVATION"
      ],
      "metadata": {
        "id": "iwfbgKeViu0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction for validation data¶"
      ],
      "metadata": {
        "id": "r81Zc6CbjXNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5\n",
        "pred_img = model.predict(VAL_XX)\n",
        "pred_img = (pred_img > threshold).astype(np.uint8)\n",
        "pred_img.shape"
      ],
      "metadata": {
        "id": "Gk_EWKpJj9hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization of validation image¶**"
      ],
      "metadata": {
        "id": "fL5t0odPknqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = 167\n",
        "fig,(ax1,ax2)= plt.subplots(1,2,figsize=(15,10))\n",
        "ax1.imshow(pred_img[img, :, :, 0])\n",
        "ax1.set_title(\"Predictions\")\n",
        "ax2.imshow(VAL_XX[img, :, :, 0:3])\n",
        "ax2.set_title('Training Image')"
      ],
      "metadata": {
        "id": "LeWP6xPHkcae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = 167\n",
        "fig,(ax1,ax2)= plt.subplots(1,2,figsize=(15,10))\n",
        "ax1.imshow(pred_img[img, :, :, 0])\n",
        "ax1.set_title(\"Predictions\")\n",
        "ax2.imshow(VAL_XX[img, :, :, 0:3])\n",
        "ax2.set_title('Training Image')\n",
        "write_directory = r'/content/gdrive/MyDrive/DL/landslide4Sense/data/validation/mask'\n",
        "for i, name in enumerate(mask_name):\n",
        "  h5f = h5py.File(write_directory + \"/\" + name, 'w')\n",
        "  # change the dimention of prediction to (n, 128, 128)\n",
        "  pred_mask = pred_img[i, :, :, 0]\n",
        "\n",
        "  # write to the directory\n",
        "  h5f.create_dataset('mask', data = pred_mask)\n",
        "  h5f.close()\n",
        "Need your help!"
      ],
      "metadata": {
        "id": "kMcfNl3jktdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uX4FQfyxnCGG"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1omgNzfR0oK33tGq_0Jw19qAIgCBjTQjA",
      "authorship_tag": "ABX9TyMTX75zbTsikQe8HnCtXzSA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}